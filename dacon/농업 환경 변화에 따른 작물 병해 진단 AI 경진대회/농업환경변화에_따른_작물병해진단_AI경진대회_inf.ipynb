{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "농업환경변화에_따른_작물병해진단_AI경진대회_inf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rUWPyJlR3j3"
      },
      "outputs": [],
      "source": [
        "# Download library ##\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "metadata": {
        "id": "IXrGuBVESgMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless==4.1.2.30"
      ],
      "metadata": {
        "id": "7VhjswZtSgQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "VCrX8GCqv6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Upload dataset ##\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test1_1.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test1_2.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test2_1.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test2_2.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test3_1.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test3_2.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test4_1.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test4_2.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test5_1.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test5_2.zip\n",
        "!unzip /content/drive/MyDrive/dacon/data/test/test6.zip"
      ],
      "metadata": {
        "id": "5K51Gbh3SgSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Library ##\n",
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import timm\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningDataModule, LightningModule\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "LUXl8Vm7SgUl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize():\n",
        "    csv_feature_dict = {\n",
        "        '내부 온도 1 평균': [3.4, 47.3],\n",
        "        '내부 온도 1 최고': [3.4, 47.6],\n",
        "        '내부 온도 1 최저': [3.3, 47.0],\n",
        "        '내부 습도 1 평균': [23.7, 100.0],\n",
        "        '내부 습도 1 최고': [25.9, 100.0],\n",
        "        '내부 습도 1 최저': [0.0, 100.0],\n",
        "        '내부 이슬점 평균': [0.1, 34.5],\n",
        "        '내부 이슬점 최고': [0.2, 34.7],\n",
        "        '내부 이슬점 최저': [0.0, 34.4]\n",
        "    }\n",
        "    \n",
        "    crop = {'1': '딸기', '2': '토마토', '3': '파프리카', '4': '오이', '5': '고추', '6': '시설포도'}\n",
        "    disease = {\n",
        "        '1': {\n",
        "            'a1': '딸기잿빛곰팡이병', 'a2': '딸기흰가루병', 'b1': '냉해피해', \n",
        "            'b6': '다량원소결핍 (N)', 'b7': '다량원소결핍 (P)', 'b8': '다량원소결핍 (K)'\n",
        "        },\n",
        "        '2': {\n",
        "            'a5': '토마토흰가루병', 'a6': '토마토잿빛곰팡이병', 'b2': '열과', 'b3': '칼슘결핍',\n",
        "            'b6': '다량원소결핍 (N)', 'b7': '다량원소결핍 (P)', 'b8': '다량원소결핍 (K)'\n",
        "        },\n",
        "        '3': {\n",
        "            'a9': '파프리카흰가루병', 'a10': '파프리카잘록병', 'b3': '칼슘결핍', \n",
        "            'b6': '다량원소결핍 (N)', 'b7': '다량원소결핍 (P)', 'b8': '다량원소결핍 (K)'\n",
        "        },\n",
        "        '4': {\n",
        "            'a3': '오이노균병', 'a4': '오이흰가루병', 'b1': '냉해피해', \n",
        "            'b6': '다량원소결핍 (N)', 'b7': '다량원소결핍 (P)', 'b8': '다량원소결핍 (K)' \n",
        "        },\n",
        "        '5': {\n",
        "            'a7': '고추탄저병', 'a8': '고추흰가루병', 'b3': '칼슘결핍', \n",
        "            'b6': '다량원소결핍 (N)', 'b7': '다량원소결핍 (P)', 'b8': '다량원소결핍 (K)'\n",
        "        },\n",
        "        '6': {'a11': '시설포도탄저병', 'a12': '시설포도노균병', 'b4': '일소피해', 'b5': '축과병'}\n",
        "    }\n",
        "    risk = {'1': '초기', '2': '중기', '3': '말기'}\n",
        "    \n",
        "    label_description = {}\n",
        "    for key, value in disease.items():\n",
        "        label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
        "        for disease_code in value:\n",
        "            for risk_code in risk:\n",
        "                label = f'{key}_{disease_code}_{risk_code}'\n",
        "                label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
        "                \n",
        "    label_encoder = {key: idx for idx, key in enumerate(label_description)}\n",
        "    label_decoder = {val: key for key, val in label_encoder.items()}\n",
        "    \n",
        "    return csv_feature_dict, label_encoder, label_decoder\n",
        "  \n",
        "\n",
        "csv_feature_dict, label_encoder, label_decoder = initialize()\n",
        "\n",
        "def accuracy_function(real, pred):    \n",
        "    real = real.cpu()\n",
        "    pred = torch.argmax(pred, dim=1).cpu()\n",
        "    score = f1_score(real, pred, average='macro')\n",
        "    return score"
      ],
      "metadata": {
        "id": "_9IHt-yvSgW1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dt(PATH):\n",
        "  test = sorted(glob(PATH))\n",
        "  return test"
      ],
      "metadata": {
        "id": "Yl9AfiI_SgZN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Config ##\n",
        "SEED = 42\n",
        "IMAGE_WIDTH = 384\n",
        "IMAGE_HEIGHT = 384\n",
        "BATCH_SIZE = 8\n",
        "CLASS_N = len(label_encoder)\n",
        "LEARNING_RATE = 1e-4\n",
        "EMBEDDING_DIM = 512\n",
        "NUM_FEATURES = len(csv_feature_dict)\n",
        "MAX_LEN = 24*6\n",
        "DROPOUT_RATE = 0.1\n",
        "EPOCHS = 10\n",
        "NUM_WORKERS = 2"
      ],
      "metadata": {
        "id": "aimUX_R2SgbL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dataset ##\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, \n",
        "        files, \n",
        "        csv_feature_dict, \n",
        "        label_encoder,\n",
        "        labels=None,\n",
        "        transforms=None,\n",
        "        mode='train',\n",
        "    ):\n",
        "        self.mode = mode\n",
        "        self.files = files\n",
        "        \n",
        "        self.csv_feature_dict = csv_feature_dict\n",
        "        \n",
        "        if files is not None:\n",
        "            self.csv_feature_check = [0]*len(self.files)\n",
        "            self.csv_features = [None]*len(self.files)\n",
        "            \n",
        "        self.max_len = 24 * 6\n",
        "        \n",
        "        self.label_encoder = label_encoder\n",
        "        \n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        file = self.files[i]\n",
        "        file_name = file.split(os.sep)[-1]\n",
        "        \n",
        "        # csv\n",
        "        if self.csv_feature_check[i] == 0:\n",
        "            csv_path = f'{file}/{file_name}.csv'\n",
        "            df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
        "            df = df.replace('-', 0)\n",
        "            # MinMax scaling\n",
        "            for col in df.columns:\n",
        "                df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
        "                df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
        "            # zero padding\n",
        "            pad = np.zeros((self.max_len, len(df.columns)))\n",
        "            length = min(self.max_len, len(df))\n",
        "            pad[-length:] = df.to_numpy()[-length:]\n",
        "            # transpose to sequential data\n",
        "            csv_feature = pad.T\n",
        "            self.csv_features[i] = csv_feature\n",
        "            self.csv_feature_check[i] = 1\n",
        "        else:\n",
        "            csv_feature = self.csv_features[i]\n",
        "        \n",
        "        # image\n",
        "        image_path = f'{file}/{file_name}.jpg'\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(image=img)['image']\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "            json_path = f'{file}/{file_name}.json'\n",
        "            with open(json_path, 'r') as f:\n",
        "                json_file = json.load(f)\n",
        "            \n",
        "            crop = json_file['annotations']['crop']\n",
        "            disease = json_file['annotations']['disease']\n",
        "            risk = json_file['annotations']['risk']\n",
        "            label = f'{crop}_{disease}_{risk}'\n",
        "            \n",
        "            return {\n",
        "                'img': img,\n",
        "                'csv_feature': torch.tensor(csv_feature, dtype=torch.float32),\n",
        "                'label': torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'img': img,\n",
        "                'csv_feature': torch.tensor(csv_feature, dtype=torch.float32)\n",
        "            }\n",
        "    \n",
        "    \n",
        "class CustomDataModule(LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        train=None,\n",
        "        val=None,\n",
        "        test=None,\n",
        "        csv_feature_dict=None,\n",
        "        label_encoder=None,\n",
        "        train_transforms=None,\n",
        "        val_transforms=None,\n",
        "        predict_transforms=None,\n",
        "        num_workers=32,\n",
        "        batch_size=8,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train = train\n",
        "        self.val = val\n",
        "        self.test = test\n",
        "        self.csv_feature_dict = csv_feature_dict\n",
        "        self.label_encoder = label_encoder\n",
        "        assert self.csv_feature_dict is not None\n",
        "        assert self.label_encoder is not None\n",
        "        self.train_transforms = train_transforms\n",
        "        self.val_transforms = val_transforms\n",
        "        self.predict_transforms = predict_transforms\n",
        "        self.num_workers = num_workers\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = CustomDataset(\n",
        "            self.train, \n",
        "            self.csv_feature_dict,\n",
        "            self.label_encoder,\n",
        "            transforms=self.train_transforms,\n",
        "        )\n",
        "        self.valid_dataset = CustomDataset(\n",
        "            self.val, \n",
        "            self.csv_feature_dict,\n",
        "            self.label_encoder,\n",
        "            transforms=self.train_transforms,\n",
        "        )\n",
        "        self.predict_dataset = CustomDataset(\n",
        "            self.test, \n",
        "            self.csv_feature_dict,\n",
        "            self.label_encoder,\n",
        "            transforms=self.predict_transforms,\n",
        "            mode='test'\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            pin_memory=True,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.valid_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            pin_memory=True,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        \n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.predict_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            pin_memory=True,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "        )"
      ],
      "metadata": {
        "id": "FMzNbCIGSgdJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model ##\n",
        "class CNN_Encoder(nn.Module):\n",
        "    def __init__(self, class_n, rate=0.1):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.model = models.efficientnet_b4(pretrained=True)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        output = self.model(inputs)\n",
        "        return output\n",
        "    \n",
        "    \n",
        "class LSTM_Decoder(nn.Module):\n",
        "    def __init__(self, max_len, embedding_dim, num_features, class_n, rate):\n",
        "        super(LSTM_Decoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(max_len, embedding_dim)\n",
        "        self.rnn_fc = nn.Linear(num_features*embedding_dim, 1000)\n",
        "        self.final_layer = nn.Linear(1000 + 1000, class_n)  # resnet out_dim + lstm out_dim\n",
        "        self.dropout = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, enc_out, dec_inp):\n",
        "        hidden, _ = self.lstm(dec_inp)\n",
        "        hidden = hidden.view(hidden.size(0), -1)\n",
        "        hidden = self.rnn_fc(hidden)\n",
        "        concat = torch.cat([enc_out, hidden], dim=1)  # enc_out + hidden \n",
        "        fc_input = concat\n",
        "        output = self.dropout((self.final_layer(fc_input)))\n",
        "        return output\n",
        "    \n",
        "    \n",
        "class BaseModel(LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cnn,\n",
        "        rnn,\n",
        "        criterion,\n",
        "        learning_rate=5e-4,\n",
        "    ):\n",
        "        super(BaseModel, self).__init__()\n",
        "        \n",
        "        self.cnn = cnn\n",
        "        self.rnn = rnn\n",
        "        self.learning_rate = learning_rate\n",
        "        self.criterion = criterion\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, img, seq):\n",
        "        cnn_output = self.cnn(img)\n",
        "        output = self.rnn(cnn_output, seq)\n",
        "        \n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        img = batch['img']\n",
        "        csv_feature = batch['csv_feature']\n",
        "        label = batch['label']\n",
        "        \n",
        "        output = self(img, csv_feature)\n",
        "        loss = self.criterion(output, label)\n",
        "        score = accuracy_function(label, output)\n",
        "        \n",
        "        self.log(\n",
        "            'train_loss', loss, prog_bar=True, logger=True\n",
        "        )\n",
        "        self.log(\n",
        "            'train_score', score, prog_bar=True, logger=True\n",
        "        )\n",
        "        \n",
        "        return {'loss': loss, 'train_score': score}        \n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        img = batch['img']\n",
        "        csv_feature = batch['csv_feature']\n",
        "        label = batch['label']\n",
        "        \n",
        "        output = self(img, csv_feature)\n",
        "        loss = self.criterion(output, label)\n",
        "        score = accuracy_function(label, output)\n",
        "        \n",
        "        self.log(\n",
        "            'val_loss', loss, prog_bar=True, logger=True\n",
        "        )\n",
        "        self.log(\n",
        "            'val_score', score, prog_bar=True, logger=True\n",
        "        )\n",
        "        \n",
        "        return {'val_loss': loss, 'val_score': score}\n",
        "    \n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        img = batch['img']\n",
        "        seq = batch['csv_feature']\n",
        "        \n",
        "        output = self(img, seq)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "class CNN2RNNModel(BaseModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_len, \n",
        "        embedding_dim, \n",
        "        num_features, \n",
        "        class_n,\n",
        "        rate=0.1,\n",
        "        learning_rate=5e-4,\n",
        "    ):\n",
        "        cnn = CNN_Encoder(class_n)\n",
        "        rnn = LSTM_Decoder(max_len, embedding_dim, num_features, class_n, rate)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        super(CNN2RNNModel, self).__init__(\n",
        "            cnn, rnn, criterion, learning_rate\n",
        "        )"
      ],
      "metadata": {
        "id": "O1AXok2wSgfn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformation ##\n",
        "def get_predict_transforms(height, width):\n",
        "    return A.Compose([\n",
        "        A.Resize(height=height, width=width),\n",
        "        A.Flip(0.5),\n",
        "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "def get_predict_transforms_aug(height, width):\n",
        "    return A.Compose([\n",
        "        A.Resize(height=height, width=width),\n",
        "        A.Flip(1),\n",
        "        A.Transpose(1),\n",
        "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "## Submission Func ##\n",
        "def get_submission(outputs, save_dir, save_filename, label_decoder, sample_sub_path):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "        \n",
        "    outputs = [o.detach().cpu().numpy() for batch in outputs\n",
        "                                        for o in batch]\n",
        "    preds = np.array([label_decoder[int(val)] for val in outputs])\n",
        "    \n",
        "    submission = pd.read_csv(sample_sub_path)\n",
        "    submission['label'] = preds\n",
        "    \n",
        "    save_file_path = os.path.join(save_dir, save_filename)\n",
        "    \n",
        "    submission.to_csv(save_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "WIkTVtdOSghz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Eval func ##\n",
        "def eval(\n",
        "    ckpt_path, \n",
        "    csv_feature_dict, \n",
        "    label_encoder, \n",
        "    label_decoder,\n",
        "    submit_save_dir,\n",
        "    submit_save_name,\n",
        "    test_file_path,\n",
        "    sample_sub_path\n",
        "):\n",
        "\n",
        "    test_data = test_dt(test_file_path)\n",
        "    \n",
        "    predict_transforms = get_predict_transforms(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    predict_transforms_aug = get_predict_transforms_aug(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "\n",
        "    data_module = CustomDataModule(\n",
        "        test=test_data,\n",
        "        csv_feature_dict=csv_feature_dict,\n",
        "        label_encoder=label_encoder,\n",
        "        predict_transforms=predict_transforms,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    data_module_aug = CustomDataModule(\n",
        "        test=test_data,\n",
        "        csv_feature_dict=csv_feature_dict,\n",
        "        label_encoder=label_encoder,\n",
        "        predict_transforms=predict_transforms_aug,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    \n",
        "    model = CNN2RNNModel(\n",
        "        max_len=24*6, \n",
        "        embedding_dim=512, \n",
        "        num_features=len(csv_feature_dict), \n",
        "        class_n=len(label_encoder),\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        gpus=1,\n",
        "        precision=16,\n",
        "    )\n",
        "\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    model.load_state_dict(ckpt['state_dict'])\n",
        "\n",
        "    test_data = [data_module, data_module_aug]\n",
        "\n",
        "    outputs = trainer.predict(model, data_module)\n",
        "    outputs_aug = trainer.predict(model, data_module_aug)\n",
        "\n",
        "    outputs = [out.numpy() for out in outputs]\n",
        "    outputs = np.array(outputs)\n",
        "    outputs_aug = [out.numpy() for out in outputs_aug]\n",
        "    outputs_aug = np.array(outputs_aug)\n",
        "\n",
        "    results = (outputs + outputs_aug) /2\n",
        "    results = [torch.Tensor(result).type(torch.int32) for result in results]\n",
        "    \n",
        "    get_submission(results, submit_save_dir, submit_save_name, label_decoder, sample_sub_path)\n"
      ],
      "metadata": {
        "id": "2OVHyO8wwWcB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## EVAL ##\n",
        "CKPT_PATH = '/content/drive/MyDrive/dacon/output/effb4_epoch=16-val_score=0.954.ckpt'\n",
        "submit_save_dir='/content/drive/MyDrive/dacon/output/submission_eff4'"
      ],
      "metadata": {
        "id": "D9X5quSPSgkA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v1_1.csv'\n",
        "test_file_path = 'test1_1/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v1_1.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "v8OefcO1SgmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v1_2.csv'\n",
        "test_file_path = 'test1_2/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v1_2.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "c9814sSNSgob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v2_1.csv'\n",
        "test_file_path = 'test2_1/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v2_1.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "xAQVSyEfSgqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v2_2.csv'\n",
        "test_file_path = 'test2_2/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v2_2.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "Ktmp_ylLSgvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v3_1.csv'\n",
        "test_file_path = 'test3_1/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v3_1.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "mU33z4AhTT_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v3_2.csv'\n",
        "test_file_path = 'test3_2/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v3_2.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "69ap9OfSTUBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v4_1.csv'\n",
        "test_file_path = 'test4_1/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v4_1.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "WHLFqWDWTUEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v4_2.csv'\n",
        "test_file_path = 'test4_2/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v4_2.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "fbmYEcTUTUGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v5_1.csv'\n",
        "test_file_path = 'test5_1/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v5_1.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "0MY4L88GTUI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v5_2.csv'\n",
        "test_file_path = 'test5_2/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v5_2.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "l93ujzWZTULG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_save_name='submission_v6.csv'\n",
        "test_file_path = 'test6/*'\n",
        "sample_sub_path = '/content/drive/MyDrive/dacon/data/sample_submission/sample_submission_v6.csv'\n",
        "\n",
        "eval(CKPT_PATH, csv_feature_dict, label_encoder, label_decoder, submit_save_dir, submit_save_name, test_file_path, sample_sub_path)    "
      ],
      "metadata": {
        "id": "I4-srv4STUNX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}